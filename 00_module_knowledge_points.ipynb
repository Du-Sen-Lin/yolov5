{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e8325a8",
   "metadata": {},
   "source": [
    "## BCE loss\n",
    "BCE (Binary Cross Entropy) 损失是用于二分类问题的损失函数，通常用于神经网络的二元分类任务中。\n",
    "\n",
    "```python\n",
    "BCE(y, y') = -[y * log(y') + (1 - y) * log(1 - y')]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a38fe508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:21:17.293524Z",
     "start_time": "2023-10-17T02:21:16.642108Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1839, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建 BCELoss\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# 示例输入和目标（实际标签）\n",
    "y = torch.tensor([1.0, 0.0, 1.0], requires_grad=True)  # 实际标签\n",
    "y_pred = torch.tensor([0.9, 0.2, 0.8], requires_grad=True)  # 模型的预测概率值\n",
    "\n",
    "# 计算 BCELoss ==> re = - (1 * math.log(0.9) + 1 * math.log(1 - 0.2) + 1*math.log(0.8)) / 3\n",
    "loss = criterion(y_pred, y)\n",
    "\n",
    "# 打印损失\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e8def6",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss (交叉熵损失) \n",
    "是一个用于多类别分类问题的损失函数，通常用于神经网络训练中。\n",
    "\n",
    "结合了 softmax 激活函数和交叉熵损失计算。\n",
    "\n",
    "nn.CrossEntropyLoss() 会在内部应用 softmax 操作，因此在传递给损失函数时，你可以直接提供模型的原始输出，而不需要手动应用 softmax。\n",
    "\n",
    "\n",
    "假设有 C 个类别，模型的输出为一个长度为 C 的向量 y_pred，实际标签为一个长度为 C 的 one-hot 编码向量 y_true，其中只有实际类别对应的位置为 1，其余位置为 0。\n",
    "\n",
    "```python\n",
    "CE(y_pred, y_true) = - Σ(y_true[i] * log(y_pred[i])) for i = 1 to C\n",
    "```\n",
    "\n",
    "**标签平滑的损失函数：nn.CrossEntropyLoss(label_smoothing=label_smoothing)**：\n",
    "\n",
    "标签平滑是一种用于减缓过拟合的技术。它通过将目标标签向其他类别的概率分布进行平滑，来减少模型对训练数据的过度拟合。\n",
    "\n",
    "传统的分类loss采用softmax loss，先对全连接层的输出计算softmax，视为各类别的置信度概率，再利用交叉熵计算损失。\n",
    "\n",
    "现在假设一个多分类任务标签是[1,0,0]，如果它本身的label的出现了问题，这对模型的伤害是非常大的，因为在训练的过程中强行学习一个非本类的样本，并且让其概率非常高，这会影响对后验概率的估计。并且有时候类与类之间的并不是毫无关联，如果鼓励输出的概率间相差过大，这会导致一定程度上的过拟合。\n",
    "\n",
    "一个较小的常数，这使得softmax损失中的概率优目标不再为1和0； 在一定程度上避免了过拟合，也缓解了错误标签带来的影响。\n",
    "\n",
    "示例见下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "94101904",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T10:29:12.336598Z",
     "start_time": "2023-10-17T10:29:12.329407Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7912)\n",
      "tensor(0.8945)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 创建 CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "criterion2 = nn.CrossEntropyLoss(label_smoothing=0.3)\n",
    "\n",
    "# 示例输入和目标（实际标签）\n",
    "y = torch.tensor([2, 0, 1], dtype=torch.long)  # 实际标签，每个值代表一个类别的索引\n",
    "y_pred = torch.tensor([[0.1, 0.2, 0.9], [0.8, 0.3, 0.2], [0.2, 0.5, 0.3]])  # 模型的原始预测值，未经过 softmax\n",
    "\n",
    "# 计算 CrossEntropyLoss\n",
    "loss = criterion(y_pred, y)\n",
    "loss2 = criterion2(y_pred, y)\n",
    "\n",
    "# 打印损失\n",
    "print(loss)\n",
    "print(loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b9dcf5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T10:29:12.502027Z",
     "start_time": "2023-10-17T10:29:12.491022Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "re = -(math.log(math.exp(0.9) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.9))) + math.log(math.exp(0.8) / (math.exp(0.8) + math.exp(0.3) + math.exp(0.2))) + math.log(math.exp(0.5) / (math.exp(0.2) + math.exp(0.5) + math.exp(0.3))))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7cceb119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T10:29:12.993302Z",
     "start_time": "2023-10-17T10:29:12.988340Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7911708456653378"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1a25699a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T10:30:57.829812Z",
     "start_time": "2023-10-17T10:30:57.822373Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.tensor([[0, 0, 1], [1, 0, 0], [0, 1, 0]])\n",
    "# label_smoothing = 0.3 ==> torch.tensor([[0.1, 0.1., 0.7+0.1], [0.7+0.1, 0.1, 0.1], [0.1, 0.7+0.1, 0.1]])\n",
    "# y_pred = torch.tensor([[0.1, 0.2, 0.9], [0.8, 0.3, 0.2], [0.2, 0.5, 0.3]])\n",
    "re2 = -((0.1 * math.log(math.exp(0.1) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.9)))) + \\\n",
    "        (0.1 * math.log(math.exp(0.2) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.9)))) + \\\n",
    "        (0.8 * math.log(math.exp(0.9) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.9)))) \\\n",
    "        + 0.8 * math.log(math.exp(0.8) / (math.exp(0.8) + math.exp(0.3) + math.exp(0.2))) + \\\n",
    "        0.1 * math.log(math.exp(0.3) / (math.exp(0.8) + math.exp(0.3) + math.exp(0.2))) + \\\n",
    "        0.1 * math.log(math.exp(0.2) / (math.exp(0.8) + math.exp(0.3) + math.exp(0.2)))\\\n",
    "        + 0.1 * math.log(math.exp(0.2) / (math.exp(0.2) + math.exp(0.5) + math.exp(0.3))) + \\\n",
    "        0.8 * math.log(math.exp(0.5) / (math.exp(0.2) + math.exp(0.5) + math.exp(0.3))) + \\\n",
    "        0.1 * math.log(math.exp(0.3) / (math.exp(0.2) + math.exp(0.5) + math.exp(0.3))) ) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c7e87def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T10:30:58.448179Z",
     "start_time": "2023-10-17T10:30:58.443241Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8945041789986711"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aafbe8",
   "metadata": {},
   "source": [
    "## softmax\n",
    "Softmax 是一个常用的激活函数，通常用于多类别分类任务的神经网络中。它将一个实数向量（通常是神经网络的输出）变换成一个概率分布。\n",
    "\n",
    "给定一个输入向量 x = [x1, x2, ..., xn]，Softmax 函数将每个元素 xi 转换成一个介于 0 到 1 之间的值，同时保证所有元素的和为 1。这种变换使得输出可以解释为属于各个类别的概率。\n",
    "\n",
    "```python\n",
    "softmax(xi) = exp(xi) / Σ(exp(xj)) for j = 1 to n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "55bb55a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:34:46.917184Z",
     "start_time": "2023-10-17T02:34:46.910867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 0.2552, 0.5139],\n",
      "        [0.4640, 0.2814, 0.2546],\n",
      "        [0.2894, 0.3907, 0.3199]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义输入张量\n",
    "x = torch.tensor([[0.1, 0.2, 0.9], [0.8, 0.3, 0.2], [0.2, 0.5, 0.3]])\n",
    "\n",
    "# 计算 softmax\n",
    "softmax_x = F.softmax(x, dim=1)\n",
    "\n",
    "print(softmax_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2811f9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:46:33.120825Z",
     "start_time": "2023-10-17T02:46:33.116006Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "re1_1 = math.exp(0.1) / (math.exp(0.1) + math.exp(0.2) + math.exp(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88285064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:46:35.706349Z",
     "start_time": "2023-10-17T02:46:35.700989Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23090892108013442"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3f929",
   "metadata": {},
   "source": [
    "##  Log-Softmax\n",
    "在实际应用中，由于指数运算可能会导致数值稳定性问题，因此通常会使用改进版的 Softmax 函数，称为 Log-Softmax，其计算如下：\n",
    "\n",
    "```python\n",
    "log_softmax(xi) = xi - log(Σ(exp(xj))) for j = 1 to n\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57c0afee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:48:59.749547Z",
     "start_time": "2023-10-17T02:48:59.743751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4657, -1.3657, -0.6657],\n",
      "        [-0.7679, -1.2679, -1.3679],\n",
      "        [-1.2398, -0.9398, -1.1398]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义输入张量\n",
    "x = torch.tensor([[0.1, 0.2, 0.9], [0.8, 0.3, 0.2], [0.2, 0.5, 0.3]])\n",
    "\n",
    "# 计算 softmax ==>re1_1 = 0.1 - math.log(math.exp(0.1) + math.exp(0.2) + math.exp(0.9))\n",
    "log_softmax_x = F.log_softmax(x, dim=1)\n",
    "\n",
    "print(log_softmax_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf2be95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:50:37.374585Z",
     "start_time": "2023-10-17T02:50:37.372471Z"
    }
   },
   "outputs": [],
   "source": [
    "re1_1 = 0.1 - math.log(math.exp(0.1) + math.exp(0.2) + math.exp(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71415ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T02:50:40.437005Z",
     "start_time": "2023-10-17T02:50:40.434404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.4657319272479288"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re1_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7aa3f7",
   "metadata": {},
   "source": [
    "## FLOPs\n",
    "FLOPs 是浮点运算次数（Floating Point Operations），通常用于衡量深度学习模型的计算复杂度。\n",
    "\n",
    "在深度学习中，每个神经网络层都包含一定数量的浮点运算，这些运算包括加法、乘法等。FLOPs用于表示一个模型执行的所有浮点运算的总数。\n",
    "\n",
    "例如，如果一个模型在前向传播过程中总共执行了10亿次浮点运算，那么它的FLOPs数就是10亿（1e9）。 FLOPs数越高，模型的计算开销就越大，需要更多的计算资源来训练和推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974214b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95f702fc",
   "metadata": {},
   "source": [
    "## FLOPS\n",
    "FLOPS（Floating Point Operations Per Second）是一个衡量计算机或计算设备性能的指标，它表示每秒钟可以执行的浮点运算次数。\n",
    "\n",
    "如果一个模型的 FLOPs 为 1000，计算机的处理能力为 10 FLOPS，那么需要的最短计算时间为 100 秒（1000 / 10 = 100）。\n",
    "\n",
    "FLOPS（Floating Point Operations Per Second）是一种计算机性能单位，用于表示每秒钟可以执行的浮点运算次数。它通常用于衡量计算机的处理速度和性能，特别是在科学计算、人工智能、大数据处理等领域。FLOPS的单位是每秒浮点运算次数，常用的前缀有k（千）、M（百万）、G（十亿）、T（万亿）等。\n",
    "\n",
    "常见的英伟达显卡的FLOPS和显存规格：\n",
    "\n",
    "```python\n",
    "NVIDIA GeForce RTX 3090 Ti \n",
    "FP32: 36.45 TFLOPS\n",
    "显存: 24 GB GDDR6X\n",
    "\n",
    "NVIDIA GeForce RTX 3090\n",
    "FP32: 35.7 TFLOPS\n",
    "FP16: 285 TFLOPS\n",
    "\n",
    "NVIDIA GeForce GTX 1080 Ti\n",
    "FLOPS: 11.34 TFLOPS\n",
    "显存: 11 GB GDDR5X\n",
    "\n",
    "NVIDIA GeForce RTX 2080 Ti\n",
    "FLOPS: 14.2 TFLOPS\n",
    "显存: 11 GB GDDR6\n",
    "\n",
    "NVIDIA Tesla V100\n",
    "FLOPS: 7.5-15.7 TFLOPS (取决于精度)\n",
    "显存: 16 GB HBM2\n",
    "\n",
    "NVIDIA Tesla P100\n",
    "FLOPS: 4.7-9.3 TFLOPS (取决于精度)\n",
    "显存: 16 GB HBM2\n",
    "\n",
    "NVIDIA Tesla T4\n",
    "FLOPS: 8.1 TFLOPS\n",
    "显存: 16 GB GDDR6\n",
    "\n",
    "NVIDIA Quadro RTX 6000\n",
    "FLOPS: 16.3 TFLOPS\n",
    "显存: 24 GB GDDR6\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3691819",
   "metadata": {},
   "source": [
    "## 参数量对应显存大小选型\n",
    "\n",
    "假设我们已知模型的参数量为100M，最大推理batchsize为32，现在需要选定适合的推理芯片显存大小。下面是一个简单的计算过程：\n",
    "首先，我们需要计算出模型在最大推理batchsize下所需要的显存大小。假设模型的输入数据大小为[h, w]，每个元素的数据类型为float32，那么模型每个batch的输入数据大小为[h, w, 32]（32为batchsize），占用显存大小为h * w * 32 * 4（4为float32的字节大小，即4个字节），同理，输出数据大小也为[h, w, 32]，占用显存大小也为h * w * 32 * 4，因此，模型在最大推理batchsize下所需要的显存大小为：\n",
    "\n",
    "```pytoon\n",
    "显存大小 = 输入数据大小 + 输出数据大小 + 模型参数大小\n",
    "显存大小 = h * w * 32 * 4 + h * w * 32 * 4 + 100M * 4\n",
    "\n",
    "```\n",
    "\n",
    "这里假设模型所有参数都是float32类型，所以模型参数大小是100M * 4，如果模型参数类型不是float32，需要相应地调整计算。\n",
    "假设我们希望显存利用率达到80%，则最终选定的显存大小为：\n",
    "\n",
    "```python\n",
    "显存大小 = (h * w * 32 * 4 + h * w * 32 * 4 + 100M * 4) / 0.8\n",
    "\n",
    "```\n",
    "一般一个参数是值一个float，也就是4个字节, 1kb=1024字节 。\n",
    "\n",
    "pytorch中的floaps与显存计算方法:\n",
    "\n",
    "```python\n",
    "pip install thop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a8f036",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-17T03:56:33.441455Z",
     "start_time": "2023-10-17T03:56:32.957421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Building model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/conda/envs/cv_env/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/root/conda/envs/cv_env/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "flops:  714206912.0 params:  61100840.0\n",
      "flops: 714.21 M, params: 61.10 M\n"
     ]
    }
   ],
   "source": [
    "# -- coding: utf-8 --\n",
    "import torch\n",
    "import torchvision\n",
    "from thop import profile\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "model = torchvision.models.alexnet(pretrained=False)\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "flops, params = profile(model, (dummy_input,))\n",
    "print('flops: ', flops, 'params: ', params)\n",
    "print('flops: %.2f M, params: %.2f M' % (flops / 1000000.0, params / 1000000.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda5d4f",
   "metadata": {},
   "source": [
    "## fuse_conv_and_bn\n",
    "模型推理时，BN层要从训练状态切换到测试状态，此时采用模型训练中近似的均值和方差。BN层最酷的地方是它可以用一个1x1卷积等效替换，更进一步地，我们可以将BN层合并到前面的卷积层中。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/110552861\n",
    "\n",
    "https://nenadmarkus.com/p/fusing-batchnorm-and-conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6c7527",
   "metadata": {},
   "source": [
    "## nn.SiLU()激活函数\n",
    "SiLU 函数将输入 x 映射到一个介于 0 和 x 之间的范围内。与 ReLU 激活函数相比，SiLU 函数的输出范围更广，可以保留更多的信息。\n",
    "\n",
    "```python\n",
    "SiLU(x) = x * sigmoid(x)\n",
    "```\n",
    "sigmoid(x) 是 Sigmoid 函数，其数学表达式如下：\n",
    "```python\n",
    "sigmoid(x) = 1 / (1 + e^(-x))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c9b8d",
   "metadata": {},
   "source": [
    "## top1_acc 与 top5_acc\n",
    "top1_acc 和 top5_acc 是评估模型性能的两个指标：\n",
    "\n",
    "举例来说，如果模型对一张图像的预测结果是：[猫、狗、飞机、汽车、鱼]，而实际上这张图的标签是“猫”，那么：\n",
    "\n",
    "Top-1 准确率为 0%（因为最高预测概率的类别不是“猫”）。\n",
    "\n",
    "Top-5 准确率为 100%（因为“猫”在前五个预测类别中）。\n",
    "\n",
    "在实际任务中，如果要求非常高的精确度，通常会关注 Top-1 准确率。但如果任务中类别众多，而且容忍模型的细微错误，可能会更关心 Top-5 准确率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab24060",
   "metadata": {},
   "source": [
    "## EMA\n",
    "通过采用指数衰减来维持变量的移动平均值。\n",
    "\n",
    "    # EMA 指数移动平均 (Exponential Moving Average, EMA)。EMA 是一种平滑技术，它对模型参数进行平均，以减少训练期间的抖动和噪声。从而提高模型的稳定性和泛化性能。\n",
    "    # 确保只有在本地或主节点（Rank为0）时才创建 EMA 模型，分布式训练时其他节点不会创建 EMA 模型。\n",
    "    # ModelEMA 创建了一个指数移动平均模型，该模型会在训练过程中持续更新\n",
    "    \n",
    "在梯度下降的过程中，会一直维护着这个影子权重，但是这个影子权重并不会参与训练。基本的假设是，模型权重在最后的n步内，会在实际的最优点处抖动，所以我们取最后n步的平均，能使得模型更加的鲁棒。\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/68748778\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704faf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
