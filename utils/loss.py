# YOLOv5 ğŸš€ by Ultralytics, AGPL-3.0 license
"""
Loss functions
"""

import torch
import torch.nn as nn

from utils.metrics import bbox_iou
from utils.torch_utils import de_parallel


def smooth_BCE(eps=0.1):  # https://github.com/ultralytics/yolov3/issues/238#issuecomment-598028441
    # return positive, negative label smoothing BCE targets
    return 1.0 - 0.5 * eps, 0.5 * eps


class BCEBlurWithLogitsLoss(nn.Module):
    # BCEwithLogitLoss() with reduced missing label effects.
    def __init__(self, alpha=0.05):
        super().__init__()
        self.loss_fcn = nn.BCEWithLogitsLoss(reduction='none')  # must be nn.BCEWithLogitsLoss()
        self.alpha = alpha

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        pred = torch.sigmoid(pred)  # prob from logits
        dx = pred - true  # reduce only missing label effects
        # dx = (pred - true).abs()  # reduce missing label and false label effects
        alpha_factor = 1 - torch.exp((dx - 1) / (self.alpha + 1e-4))
        loss *= alpha_factor
        return loss.mean()


class FocalLoss(nn.Module):
    """åŒ…è£…äº†ä¸€ä¸ªç°æœ‰çš„æŸå¤±å‡½æ•°(loss_fcn)ï¼Œå¹¶å¯¹åŸå§‹æŸå¤±å‡½æ•°çš„è¾“å‡ºåº”ç”¨äº†Focal Lossä¿®æ­£ã€‚
    """
    # Wraps focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)
        # p_t = torch.exp(-loss)
        # loss *= self.alpha * (1.000001 - p_t) ** self.gamma  # non-zero power for gradient stability

        # TF implementation https://github.com/tensorflow/addons/blob/v0.7.1/tensorflow_addons/losses/focal_loss.py
        pred_prob = torch.sigmoid(pred)  # prob from logits
        p_t = true * pred_prob + (1 - true) * (1 - pred_prob)
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = (1.0 - p_t) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class QFocalLoss(nn.Module):
    # Wraps Quality focal loss around existing loss_fcn(), i.e. criteria = FocalLoss(nn.BCEWithLogitsLoss(), gamma=1.5)
    def __init__(self, loss_fcn, gamma=1.5, alpha=0.25):
        super().__init__()
        self.loss_fcn = loss_fcn  # must be nn.BCEWithLogitsLoss()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = loss_fcn.reduction
        self.loss_fcn.reduction = 'none'  # required to apply FL to each element

    def forward(self, pred, true):
        loss = self.loss_fcn(pred, true)

        pred_prob = torch.sigmoid(pred)  # prob from logits
        alpha_factor = true * self.alpha + (1 - true) * (1 - self.alpha)
        modulating_factor = torch.abs(true - pred_prob) ** self.gamma
        loss *= alpha_factor * modulating_factor

        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        else:  # 'none'
            return loss


class ComputeLoss:
    sort_obj_iou = False

    # Compute losses
    def __init__(self, model, autobalance=False):
        device = next(model.parameters()).device  # get model device
        h = model.hyp  # hyperparameters

        # Define criteria
        """pos_weight æ˜¯ç”¨äºè°ƒæ•´æ­£ç±»åˆ«çš„æƒé‡çš„å‚æ•°ã€‚åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­ï¼Œé€šå¸¸æƒ…å†µä¸‹è´Ÿç±»åˆ«ï¼ˆnegative classï¼‰çš„æ ·æœ¬æ•°é‡è¿œè¿œå¤§äºæ­£ç±»åˆ«ï¼ˆpositive classï¼‰çš„æ ·æœ¬æ•°é‡ã€‚
        è¿™æ ·ä¸€æ¥ï¼Œæ¨¡å‹å¯èƒ½ä¼šæ›´å€¾å‘äºé¢„æµ‹ä¸ºè´Ÿç±»åˆ«ï¼Œå› ä¸ºè¿™æ ·å¯ä»¥é™ä½æ•´ä½“çš„æŸå¤±ã€‚
        """
        BCEcls = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['cls_pw']], device=device)) # åˆ†ç±»æŸå¤±çš„æŸå¤±å‡½æ•° nn.BCEWithLogitsLossï¼Œè¿™æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»äº¤å‰ç†µæŸå¤±å‡½æ•°
        BCEobj = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([h['obj_pw']], device=device)) # ç›®æ ‡æŸå¤±çš„æŸå¤±å‡½æ•° pos_weight å‚æ•°ç”¨äºå¯¹æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„æƒé‡è¿›è¡ŒåŠ æƒï¼Œè¿™åœ¨å¤„ç†æ ·æœ¬ä¸å‡è¡¡çš„æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ã€‚

        # Class label smoothing https://arxiv.org/pdf/1902.04103.pdf eqn 3  ç±»åˆ«æ ‡ç­¾å¹³æ»‘ (Class Label Smoothing):ä¸€ç§æ­£åˆ™åŒ–æŠ€å·§ï¼Œç”¨äºè®­ç»ƒåˆ†ç±»æ¨¡å‹æ—¶å‡ç¼“æ¨¡å‹çš„è¿‡æ‹Ÿåˆ
        self.cp, self.cn = smooth_BCE(eps=h.get('label_smoothing', 0.0))  # positive, negative BCE targets æ ¹æ®è¶…å‚æ•°ä¸­çš„è®¾å®šï¼Œè¿›è¡Œäº†ç±»åˆ«å¹³æ»‘å¤„ç†ã€‚  label_smoothing æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œç”¨äºæ§åˆ¶å¹³æ»‘çš„ç¨‹åº¦
        # self.cp å’Œ self.cn åˆ†åˆ«æ˜¯ç”¨äºå¹³æ»‘çš„æ­£ç±»åˆ«å’Œè´Ÿç±»åˆ«çš„ BCEï¼ˆäºŒå…ƒäº¤å‰ç†µï¼‰ç›®æ ‡ã€‚

        # Focal loss https://arxiv.org/pdf/1708.02002.pdf ä¸€ç§æŸå¤±å‡½æ•°ï¼Œæ—¨åœ¨è§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œå°¤å…¶æ˜¯åœ¨å­˜åœ¨å¤§é‡è´Ÿæ ·æœ¬çš„æƒ…å†µä¸‹ã€‚
        g = h['fl_gamma']  # focal loss gamma fl_gamma æ˜¯ Focal Loss ä¸­çš„ä¸€ä¸ªè¶…å‚æ•°ï¼Œæ§åˆ¶äº†æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ï¼Œé€šå¸¸è®¾ä¸ºæ­£å€¼ä»¥é™ä½æ˜“åˆ†ç±»æ ·æœ¬çš„æƒé‡ã€‚
        if g > 0: # å¦‚æœ fl_gamma å¤§äº 0ï¼Œåˆ™ä¼šåˆ›å»ºä¸€ä¸ª Focal Loss çš„å®ä¾‹ï¼Œå¹¶å°†å…¶ç”¨äºåˆ†ç±»æŸå¤± BCEcls å’Œç›®æ ‡æŸå¤± BCEobj ä¸­
            BCEcls, BCEobj = FocalLoss(BCEcls, g), FocalLoss(BCEobj, g)

        m = de_parallel(model).model[-1]  # Detect() module ä»ä¼ å…¥çš„æ¨¡å‹ model ä¸­è·å–æœ€åä¸€å±‚ï¼Œè¿™é‡Œæ˜¯ä¸€ä¸ªDetect()æ¨¡å—ã€‚
        self.balance = {3: [4.0, 1.0, 0.4]}.get(m.nl, [4.0, 1.0, 0.25, 0.06, 0.02])  # P3-P7
        self.ssi = list(m.stride).index(16) if autobalance else 0  # stride 16 index
        self.BCEcls, self.BCEobj, self.gr, self.hyp, self.autobalance = BCEcls, BCEobj, 1.0, h, autobalance
        self.na = m.na  # number of anchors
        self.nc = m.nc  # number of classes
        self.nl = m.nl  # number of layers
        self.anchors = m.anchors # anchors è¡¨ç¤ºé”šæ¡†çš„å°ºå¯¸ï¼Œå®ƒç­‰äº m.anchorsï¼Œå³ä»æ¨¡å‹ m ä¸­è·å–çš„é”šæ¡†å°ºå¯¸ã€‚
        print(f"-------- m.anchors: {m.anchors}")
        self.device = device

    def __call__(self, p, targets):  # predictions, targets
        """ç±»çš„ä¸»è¦è°ƒç”¨å‡½æ•°ï¼Œç”¨äºè®¡ç®—æ€»çš„æŸå¤±ã€‚

        Args:
            p (_type_): æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            targets (_type_): çœŸå®æ ‡ç­¾

        Returns:
            _type_: è¿”å›åŠ æƒåçš„æ€»æŸå¤±
        """
        lcls = torch.zeros(1, device=self.device)  # class loss
        lbox = torch.zeros(1, device=self.device)  # box loss
        lobj = torch.zeros(1, device=self.device)  # object loss
        tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets è°ƒç”¨build_targetså‡½æ•°æ„å»ºç›®æ ‡

        # Losses
        for i, pi in enumerate(p):  # layer index, layer predictions éå†æ¯ä¸ªé¢„æµ‹å±‚ï¼Œè®¡ç®—åˆ†ç±»æŸå¤±ã€ç›®æ ‡æ¡†æŸå¤±å’Œç›®æ ‡æŸå¤±ï¼Œå¹¶ç´¯åŠ åˆ°å¯¹åº”çš„å˜é‡ä¸­ã€‚
            print(f"---------------i: {i}, pi: {pi.shape}")
            """ è®­ç»ƒæ—¶
            ---------------i: 0, pi: torch.Size([16, 3, 80, 80, 10]) # æ£€æµ‹å°ç›®æ ‡ 16 batch_size; 80x80 ç‰¹å¾å›¾å—; 3 æ¯ä¸ªç‰¹å¾å›¾å—é¢„æµ‹ä¸‰ä¸ªæ¡†; 10=5(ç±»åˆ«)+4(ä½ç½®)+1(ç›®æ ‡ç½®ä¿¡åº¦)
            ---------------i: 1, pi: torch.Size([16, 3, 40, 40, 10])
            ---------------i: 2, pi: torch.Size([16, 3, 20, 20, 10]) # æ£€æµ‹å¤§ç›®æ ‡
            """
            """ æ¨¡å‹è¯„ä¼°ï¼ˆæ¨æ–­ï¼‰
            ---------------i: 0, pi: torch.Size([32, 3, 64, 84, 10])
            ---------------i: 1, pi: torch.Size([32, 3, 32, 42, 10])
            ---------------i: 2, pi: torch.Size([32, 3, 16, 21, 10])  
            """
            b, a, gj, gi = indices[i]  # image, anchor, gridy, gridx
            tobj = torch.zeros(pi.shape[:4], dtype=pi.dtype, device=self.device)  # target obj

            n = b.shape[0]  # number of targets
            if n:
                # pxy, pwh, _, pcls = pi[b, a, gj, gi].tensor_split((2, 4, 5), dim=1)  # faster, requires torch 1.8.0
                #  pxyï¼ˆé¢„æµ‹çš„ä¸­å¿ƒåæ ‡ï¼‰ã€pwhï¼ˆé¢„æµ‹çš„å®½é«˜ï¼‰ã€_ï¼ˆè¿™ä¸ªéƒ¨åˆ†åœ¨è®¡ç®—ç›®æ ‡æ¡†æŸå¤±æ—¶æ²¡æœ‰ç”¨åˆ°ï¼‰ã€pclsï¼ˆé¢„æµ‹çš„ç±»åˆ«ï¼‰
                pxy, pwh, _, pcls = pi[b, a, gj, gi].split((2, 2, 1, self.nc), 1)  # target-subset of predictions

                # Regression ç›®æ ‡æ¡†æŸå¤±
                pxy = pxy.sigmoid() * 2 - 0.5 # å¯¹é¢„æµ‹çš„ä¸­å¿ƒåæ ‡è¿›è¡Œäº†å˜æ¢ã€‚å®ƒé¦–å…ˆå¯¹åæ ‡è¿›è¡Œäº† sigmoid æ“ä½œï¼Œå°†å…¶èŒƒå›´é™åˆ¶åœ¨ (0, 1) ä¹‹é—´ï¼Œç„¶åä¹˜ä»¥ 2 å¹¶å‡å» 0.5ï¼Œå°†èŒƒå›´æ˜ å°„åˆ° (-0.5, 0.5)ã€‚
                pwh = (pwh.sigmoid() * 2) ** 2 * anchors[i] # å¯¹é¢„æµ‹çš„å®½é«˜è¿›è¡Œäº†å˜æ¢ã€‚ç±»ä¼¼äºä¸­å¿ƒåæ ‡ï¼Œé¦–å…ˆè¿›è¡Œäº† sigmoid æ“ä½œï¼Œç„¶åä¹˜ä»¥ 2 çš„å¹³æ–¹ï¼Œæœ€åå†ä¹˜ä»¥å¯¹åº”çš„é”šæ¡†å°ºå¯¸ã€‚
                pbox = torch.cat((pxy, pwh), 1)  # predicted box å°†å¤„ç†åçš„ä¸­å¿ƒåæ ‡å’Œå®½é«˜æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œå¾—åˆ°äº†é¢„æµ‹çš„ç›®æ ‡æ¡†ã€‚
                iou = bbox_iou(pbox, tbox[i], CIoU=True).squeeze()  # iou(prediction, target) è®¡ç®—äº†é¢„æµ‹æ¡†å’Œç›®æ ‡æ¡†ä¹‹é—´çš„ IoUï¼ˆäº¤å¹¶æ¯”ï¼‰
                lbox += (1.0 - iou).mean()  # iou loss è®¡ç®—äº†ç›®æ ‡æ¡†å›å½’çš„æŸå¤±ã€‚å®ƒå°† 1 å‡å» IoU å¾—åˆ°çš„å€¼ï¼Œç„¶åå–å¹³å‡ï¼Œç´¯åŠ åˆ° lbox å˜é‡ä¸­ã€‚

                # Objectness ç›®æ ‡æŸå¤±
                iou = iou.detach().clamp(0).type(tobj.dtype)
                if self.sort_obj_iou:
                    j = iou.argsort()
                    b, a, gj, gi, iou = b[j], a[j], gj[j], gi[j], iou[j]
                if self.gr < 1:
                    iou = (1.0 - self.gr) + self.gr * iou
                tobj[b, a, gj, gi] = iou  # iou ratio å°†å¤„ç†åçš„ IoU å€¼èµ‹å€¼ç»™ tobjï¼Œå³ç›®æ ‡å¼ é‡ï¼Œç”¨äºè®¡ç®—ç›®æ ‡æŸå¤±ã€‚

                # Classification åˆ†ç±»æŸå¤±
                if self.nc > 1:  # cls loss (only if multiple classes) åªæœ‰åœ¨æœ‰å¤šç±»åˆ«æ—¶æ‰è®¡ç®—åˆ†ç±»æŸå¤±ã€‚
                    t = torch.full_like(pcls, self.cn, device=self.device)  # targets åˆ›å»ºä¸€ä¸ªä¸é¢„æµ‹çš„ç±»åˆ«å¼ é‡ pcls æœ‰ç›¸åŒå½¢çŠ¶çš„å…¨1å¼ é‡ä½œä¸º targetsã€‚self.cn æ˜¯è´Ÿç±»åˆ«æƒé‡ï¼Œself.cp æ˜¯æ­£ç±»åˆ«æƒé‡ã€‚
                    t[range(n), tcls[i]] = self.cp
                    lcls += self.BCEcls(pcls, t)  # BCE äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°è®¡ç®—åˆ†ç±»æŸå¤±ã€‚

                # Append targets to text file
                # with open('targets.txt', 'a') as file:
                #     [file.write('%11.5g ' * 4 % tuple(x) + '\n') for x in torch.cat((txy[i], twh[i]), 1)]

            obji = self.BCEobj(pi[..., 4], tobj) # è®¡ç®—ç›®æ ‡ç½®ä¿¡åº¦çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±ã€‚
            lobj += obji * self.balance[i]  # obj loss å°†æŸå¤±ä¹˜ä»¥ç›¸åº”çš„æƒé‡ï¼Œç„¶ååŠ åˆ°æ€»çš„ç›®æ ‡ç½®ä¿¡åº¦æŸå¤±ä¸­ã€‚
            if self.autobalance: # æ£€æŸ¥æ˜¯å¦éœ€è¦è‡ªåŠ¨å¹³è¡¡æƒé‡
                self.balance[i] = self.balance[i] * 0.9999 + 0.0001 / obji.detach().item()

        if self.autobalance:
            self.balance = [x / self.balance[self.ssi] for x in self.balance]
        lbox *= self.hyp['box'] # æŸå¤±ä¹˜ä»¥è¶…å‚æ•°ä¸­çš„æƒé‡
        lobj *= self.hyp['obj']
        lcls *= self.hyp['cls']
        bs = tobj.shape[0]  # batch size

        return (lbox + lobj + lcls) * bs, torch.cat((lbox, lobj, lcls)).detach()

    def build_targets(self, p, targets):
        """æ„å»ºç›®æ ‡

        Args:
            p (_type_): æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            targets (_type_): çœŸå®æ ‡ç­¾

        Returns:
            _type_: _description_
        """
        # Build targets for compute_loss(), input targets(image,class,x,y,w,h) æ ¹æ® anchor æ•°é‡å’Œç›®æ ‡æ•°é‡åˆå§‹åŒ–äº†ç”¨äºä¿å­˜åˆ†ç±»ã€ç›®æ ‡æ¡†ã€ç´¢å¼•å’Œé”šæ¡†ä¿¡æ¯çš„åˆ—è¡¨
        na, nt = self.na, targets.shape[0]  # number of anchors, targets na è¡¨ç¤ºé”šæ¡†çš„æ•°é‡ï¼Œnt è¡¨ç¤ºç›®æ ‡çš„æ•°é‡
        tcls, tbox, indices, anch = [], [], [], [] # ä¿å­˜åˆ†ç±»ã€ç›®æ ‡æ¡†ã€ç´¢å¼•å’Œé”šæ¡†ä¿¡æ¯ã€‚
        gain = torch.ones(7, device=self.device)  # normalized to gridspace gain gridç©ºé—´çš„å¢ç›Š gainï¼Œå°†ç›®æ ‡åæ ‡æ˜ å°„åˆ° grid ç©ºé—´
        ai = torch.arange(na, device=self.device).float().view(na, 1).repeat(1, nt)  # same as .repeat_interleave(nt) ai å¼ é‡ï¼Œå®ƒæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (na, nt) çš„å¼ é‡ï¼Œç”¨äºå­˜å‚¨é”šæ¡†çš„ç´¢å¼•ã€‚
        targets = torch.cat((targets.repeat(na, 1, 1), ai[..., None]), 2)  # append anchor indices ç›®æ ‡å¼ é‡ targets ä¸ ai æ°´å¹³æ‹¼æ¥ï¼Œå½¢æˆä¸€ä¸ªå½¢çŠ¶ä¸º (na * nt, 7) çš„å¼ é‡ï¼Œå…¶ä¸­åŒ…å«äº†ç›®æ ‡çš„ç±»åˆ«ã€åæ ‡å’Œé”šæ¡†çš„ç´¢å¼•ä¿¡æ¯ã€‚
        # print(f"--------------gain :{gain.shape},  --- ai: {ai.shape}, ----- targets: {targets.shape}")
        # iteration1 --------------gain :torch.Size([7]),  --- ai: torch.Size([3, 16]), ----- targets: torch.Size([3, 16, 7])
        # iteration2 --------------gain :torch.Size([7]),  --- ai: torch.Size([3, 14]), ----- targets: torch.Size([3, 14, 7])
        # ... ntæ˜¯ä¼šå˜åŒ–çš„
        # iteration7 --------------gain :torch.Size([7]),  --- ai: torch.Size([3, 23]), ----- targets: torch.Size([3, 23, 7])

        g = 0.5  # bias åˆå§‹åŒ–äº†ä¸€ä¸ªåç§»é‡ offï¼Œå®ƒæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º (5, 2) çš„å¼ é‡ï¼Œç”¨äºè®¡ç®—ç›®æ ‡çš„åç§»
        off = torch.tensor(
            [
                [0, 0],
                [1, 0],
                [0, 1],
                [-1, 0],
                [0, -1],  # j,k,l,m
                # [1, 1], [1, -1], [-1, 1], [-1, -1],  # jk,jm,lk,lm
            ],
            device=self.device).float() * g  # offsets
        # print(f"------------- self.nl: {self.nl}")
        # ------------- self.nl: 3
        for i in range(self.nl): # éå†æ¨¡å‹çš„ä¸åŒå±‚ï¼Œå¯¹æ¯ä¸€å±‚è¿›è¡Œå¤„ç†
            anchors, shape = self.anchors[i], p[i].shape
            gain[2:6] = torch.tensor(shape)[[3, 2, 3, 2]]  # xyxy gain

            # Match targets to anchors å°†ç›®æ ‡åæ ‡ä¸é”šæ¡†çš„å®½é«˜æ¯”è¿›è¡Œæ¯”è¾ƒ
            t = targets * gain  # shape(3,n,7)
            if nt:
                # Matches
                r = t[..., 4:6] / anchors[:, None]  # wh ratio
                j = torch.max(r, 1 / r).max(2)[0] < self.hyp['anchor_t']  # compare  å¸ƒå°”å¼ é‡ jï¼Œè¡¨ç¤ºç›®æ ‡æ˜¯å¦ä¸é”šæ¡†åŒ¹é…ã€‚
                # j = wh_iou(anchors, t[:, 4:6]) > model.hyp['iou_t']  # iou(3,n)=wh_iou(anchors(3,2), gwh(n,2))
                t = t[j]  # filter æ ¹æ® j å¯¹ç›®æ ‡è¿›è¡Œè¿‡æ»¤

                # Offsets è®¡ç®—ç›®æ ‡åœ¨ grid ç©ºé—´çš„åæ ‡ï¼Œå¹¶æ ¹æ®åç§»é‡ off è®¡ç®—ç›®æ ‡çš„åç§»
                gxy = t[:, 2:4]  # grid xy
                gxi = gain[[2, 3]] - gxy  # inverse
                j, k = ((gxy % 1 < g) & (gxy > 1)).T
                l, m = ((gxi % 1 < g) & (gxi > 1)).T
                j = torch.stack((torch.ones_like(j), j, k, l, m))
                t = t.repeat((5, 1, 1))[j]
                offsets = (torch.zeros_like(gxy)[None] + off[:, None])[j]
            else:
                t = targets[0]
                offsets = 0

            # Define
            bc, gxy, gwh, a = t.chunk(4, 1)  # (image, class), grid xy, grid wh, anchors
            a, (b, c) = a.long().view(-1), bc.long().T  # anchors, image, class
            gij = (gxy - offsets).long()
            gi, gj = gij.T  # grid indices

            # Append ç›®æ ‡çš„ç±»åˆ«ã€åæ ‡ã€ç´¢å¼•å’Œé”šæ¡†ä¿¡æ¯
            indices.append((b, a, gj.clamp_(0, shape[2] - 1), gi.clamp_(0, shape[3] - 1)))  # image, anchor, grid
            tbox.append(torch.cat((gxy - gij, gwh), 1))  # box
            anch.append(anchors[a])  # anchors
            tcls.append(c)  # class

        return tcls, tbox, indices, anch
